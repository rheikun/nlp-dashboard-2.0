{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mengimport Library yang Dibutuhkan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords as stopwords_scratch\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Membuat dataframe stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Panggil stopword ID\n",
    "list_stopwords = stopwords_scratch.words('indonesian')\n",
    "# Panggil stopword EN\n",
    "list_stopwords_en = stopwords_scratch.words('english')\n",
    "# Gabungkan ID & EN\n",
    "list_stopwords.extend(list_stopwords_en)\n",
    "# Tambah daftar stopword jika perlu\n",
    "list_stopwords.extend(['ya', 'yg', 'ga', 'aja', 'yuk', 'dah', 'ngga', 'engga', 'ygy', 'gak', 'nya', 'baiknya', 'berkali', 'kali', 'kurangnya', 'mata', 'olah', 'sekurang', 'setidak', 'tama', 'tidaknya'])\n",
    "\n",
    "# Buat DataFrame dari list stopwords\n",
    "stopwords_df = pd.DataFrame(list_stopwords, columns=['stopword'])\n",
    "\n",
    "# Simpan DataFrame sebagai CSV\n",
    "stopwords_df.to_csv('./data/stopwords.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mengimport dataframe yang dibutuhkan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df = pd.read_csv(\"./data/dataset_comment_yt.csv\")\n",
    "stopword_df = pd.read_csv(\"./data/stopwords.csv\", header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Menginisialiasi Stemmer untuk bahasa Indonesia (mengubah kata ke bentuk dasar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Membuat method pre processing text untuk membersihkan dataset dari emoji, karakter khusus, membuat jadi lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi pre-processing\n",
    "def preprocess_text(text):\n",
    "    # Hapus emoji\n",
    "    text = emoji.replace_emoji(text, replace=\"\")\n",
    "    \n",
    "    # Hapus nama orang (pola kapitalisasi)\n",
    "    text = re.sub(r'\\b[A-Z][a-z]*\\b', '', text)\n",
    "    \n",
    "    # Hapus karakter khusus, angka, dan URL\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+|[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Konversi ke lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stopword_df]\n",
    "    \n",
    "    # Remove very short words (kurang dari 2 karakter)\n",
    "    tokens = [word for word in tokens if len(word) > 2]\n",
    "    \n",
    "    # Stemming\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    # Gabungkan kembali token menjadi string\n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Menerapkan method preprocess_text ke dalam df pada kolom 'comment' yang disimpan menjadi kolom 'cleaned_comment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              comment  \\\n",
      "4                             Gibraaaann MANTAPKUðŸ”¥ðŸ”¥ðŸ”¥ðŸ”¥   \n",
      "5   Pengen balik ke sini.  Lihat orang2 yg dulu ng...   \n",
      "8             Woi Tapera Tapera ok gas ok gas pretttt   \n",
      "9                               AkademisiRocky Gerung   \n",
      "10  uustadzRockyvGerungprofDrAhliFilsapatnterkemuk...   \n",
      "\n",
      "                                      cleaned_comment  \n",
      "4                                              mantap  \n",
      "5   balik ke sini orang yg dulu ngalem bocil msh t...  \n",
      "8                               ok gas ok gas pretttt  \n",
      "9                                      akademisirocky  \n",
      "10  uustadzrockyvgerungprofdrahlifilsapatnterkemukadi  \n"
     ]
    }
   ],
   "source": [
    "comment_df['comment'] = comment_df['comment'].astype(str)\n",
    "comment_df['cleaned_comment'] = comment_df['comment'].apply(preprocess_text)\n",
    "\n",
    "comment_df = comment_df[comment_df['cleaned_comment'].str.strip() != '']\n",
    "\n",
    "# Tampilkan hasil pre-processing pada beberapa baris pertama\n",
    "print(comment_df[['comment', 'cleaned_comment']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Menghapus data duplikat yang ada di kolom 'cleaned_comment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah data duplikat: 1719\n"
     ]
    }
   ],
   "source": [
    "print(\"Jumlah data duplikat:\", comment_df['cleaned_comment'].duplicated().sum())\n",
    "\n",
    "comment_df = comment_df.drop_duplicates(subset='cleaned_comment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Menyimpan data yang telah dibersihkan ke dalam bentuk file CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df.to_csv('./data/cleaned_comment_yt.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
